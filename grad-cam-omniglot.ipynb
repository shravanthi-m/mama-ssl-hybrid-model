{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":110433,"sourceType":"datasetVersion","datasetId":57364},{"sourceId":10081877,"sourceType":"datasetVersion","datasetId":6215546}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"train_data_path = '/kaggle/input/omniglot/images_background/'\ntest_data_path = '/kaggle/input/omniglot/images_evaluation/'\nn_way = 5\n# n_way = 20\nk_shot = 1\nq_query = 5\nouter_lr = 0.001\ninner_lr = 0.04\nmeta_batch_size = 32\ntrain_inner_step = 1\neval_inner_step = 3\nnum_iterations = 1000\n# num_iterations = 500\nnum_workers = 0\nvalid_size = 0.2\nrandom_seed = 42\ndisplay_gap = 50","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport random\nimport collections\nimport numpy as np\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torch import nn\nimport torchvision\nfrom torch.utils.data import DataLoader, Subset\nfrom torch.utils.data import random_split\nfrom torch.utils.data.dataset import Dataset\nfrom torchvision.transforms import transforms\nfrom torchvision import datasets, transforms\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nrandom.seed(random_seed)\nnp.random.seed(random_seed)\ntorch.manual_seed(random_seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(random_seed)\nos.environ['PYTHONHASHSEED'] = str(random_seed)\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MAMLDataset(Dataset):\n    # here 'task_num' is the batch size of each meta learning iteration\n    # \"n_way\" is the number of classes to be classified\n    # \"k_shot\" is the number of data samples of each class used for training (support)\n    # \"q_query\" is the number of data samples of each class used for testing (query)\n    # \"k_shot\" is set to be equal to \"q_query\" by default\n    def __init__(self, data_path, n_way=5, k_shot=1, q_query=1):\n\n        self.file_list = self.get_file_list(data_path)\n        self.n_way = n_way\n        self.k_shot = k_shot\n        self.q_query = q_query\n\n    def get_file_list(self, data_path):\n        raise NotImplementedError('get_file_list function not implemented!')\n\n    def get_one_task_data(self):\n        raise NotImplementedError('get_one_task_data function not implemented!')\n\n    def __len__(self):\n        # This number does not influence the results,\n        # since we will randomly sample task \n        # from the entire class dataset each time,\n        # which means that the size of it can be seen as Infinity.\n        # And as the setting below, any number here wont influence the result.\n        return len(self.file_list)\n\n    def __getitem__(self, index):\n        return self.get_one_task_data()\n\nclass OmniglotDataset(MAMLDataset):\n    def get_file_list(self, data_path):\n        \"\"\"\n        Get a list of all classes.\n        Args:\n            data_path: Omniglot Data path\n\n        Returns: list of all classes\n\n        \"\"\"\n        return [f for f in glob.glob(data_path + \"**/character*\", recursive=True)]\n\n    def get_one_task_data(self):\n        \"\"\"\n        Get ones task maml data, include one batch support images and labels, one batch query images and labels.\n        Returns: support_data, query_data\n\n        \"\"\"\n        img_dirs = random.sample(self.file_list, self.n_way)\n        support_data = []\n        query_data = []\n\n        support_image = []\n        support_label = []\n        query_image = []\n        query_label = []\n\n        for label, img_dir in enumerate(img_dirs):\n            img_list = [f for f in glob.glob(img_dir + \"**/*.png\", recursive=True)]\n            images = random.sample(img_list, self.k_shot + self.q_query)\n\n            # Read support set\n            for img_path in images[:self.k_shot]:\n                image = transforms.Resize(size=28)(Image.open(img_path))\n                image = np.array(image)\n                image = np.expand_dims(image / 255., axis=0)\n                support_data.append((image, label))\n\n            # Read query set\n            for img_path in images[self.k_shot:]:\n                image = transforms.Resize(size=28)(Image.open(img_path))\n                image = np.array(image)\n                image = np.expand_dims(image / 255., axis=0)\n                query_data.append((image, label))\n\n        # shuffle support set\n        random.shuffle(support_data)\n        for data in support_data:\n            support_image.append(data[0])\n            support_label.append(data[1])\n\n        # shuffle query set\n        random.shuffle(query_data)\n        for data in query_data:\n            query_image.append(data[0])\n            query_label.append(data[1])\n\n        # query_label = np.array(query_label)\n\n        # # Now apply pseudo-labeling for unlabeled data (query set)\n        # query_tensor = torch.tensor(query_image).float()\n        \n        # # Generate pseudo-labels for unlabeled data in the query set\n        # pseudo_labels, high_confidence = generate_pseudo_labels(self.classifier, query_tensor)\n\n        # # Update labels for high-confidence pseudo-labeled samples\n        # query_label[high_confidence] = pseudo_labels[high_confidence]\n\n        return np.array(support_image), np.array(support_label), np.array(query_image), np.array(query_label)\n\n\nclass OmniglotDatasetTrain(MAMLDataset):\n    def __init__(self, data_path, n_way=5, k_shot=1, q_query=1, classifier=None):\n        super().__init__(data_path, n_way, k_shot, q_query)\n        self.classifier = classifier\n    def get_file_list(self, data_path):\n        \"\"\"\n        Get a list of all classes.\n        Args:\n            data_path: Omniglot Data path\n\n        Returns: list of all classes\n\n        \"\"\"\n        return [f for f in glob.glob(data_path + \"**/character*\", recursive=True)]\n        \n    def get_one_task_data(self):\n        \"\"\"\n        Generate one MAML task, include one batch of support images and labels, one batch of query images and labels.\n        \"\"\"\n        img_dirs = random.sample(self.file_list, self.n_way)\n        support_data = []\n        query_data = []\n\n        support_images = []\n        support_labels = []\n        query_images = []\n        query_labels = []\n\n        for label, img_dir in enumerate(img_dirs):\n            img_list = [f for f in glob.glob(img_dir + \"**/*.png\", recursive=True)]\n            images = random.sample(img_list, self.k_shot + self.q_query)\n\n            # Read support set\n            for img_path in images[:self.k_shot]:\n                image = transforms.Resize(size=28)(Image.open(img_path))\n                image = np.array(image)\n                image = np.expand_dims(image / 255., axis=0)\n                support_data.append((image, label))\n\n            # Read query set\n            for img_path in images[self.k_shot:]:\n                image = transforms.Resize(size=28)(Image.open(img_path))\n                image = np.array(image)\n                image = np.expand_dims(image / 255., axis=0)\n                query_data.append((image, label))\n\n        # shuffle support set\n        random.shuffle(support_data)\n        for data in support_data:\n            support_images.append(data[0])\n            support_labels.append(data[1])\n\n        # shuffle query set\n        random.shuffle(query_data)\n        for data in query_data:\n            query_images.append(data[0])\n            query_labels.append(data[1])\n\n        # Convert to Tensor (before passing to model)\n        support_images = torch.tensor(support_images).float()\n        support_labels = torch.tensor(support_labels).long()\n\n        query_images = torch.tensor(query_images).float()\n        # query_labels = torch.tensor(query_labels).long()\n\n        # Convert grayscale images (1 channel) to RGB (3 channels)\n        query_images_rgb = query_images.repeat(1, 3, 1, 1)  # Replicating grayscale to RGB\n\n        # Now query_images_rgb has the right shape for ResNet (3 channels)\n        logits = self.classifier(query_images_rgb)  # Generate pseudo-labels using ResNet\n\n        # Generate pseudo-labels based on the model's predictions\n        pseudo_labels, high_confidence = generate_pseudo_labels(self.classifier, query_images_rgb)\n\n        query_labels = np.array(query_labels)\n        pseudo_labels = np.array(pseudo_labels)\n\n        # Use the high-confidence pseudo-labels to update query_labels\n        query_labels[high_confidence] = pseudo_labels[high_confidence]\n\n        return np.array(support_images), np.array(support_labels), np.array(query_images), np.array(query_labels)\n\n    # def get_one_task_data(self):\n    #     \"\"\"\n    #     Get ones task maml data, include one batch support images and labels, one batch query images and labels.\n    #     Returns: support_data, query_data\n\n    #     \"\"\"\n    #     img_dirs = random.sample(self.file_list, self.n_way)\n    #     support_data = []\n    #     query_data = []\n\n    #     support_image = []\n    #     support_label = []\n    #     query_image = []\n    #     query_label = []\n\n    #     for label, img_dir in enumerate(img_dirs):\n    #         img_list = [f for f in glob.glob(img_dir + \"**/*.png\", recursive=True)]\n    #         images = random.sample(img_list, self.k_shot + self.q_query)\n\n    #         # Read support set\n    #         for img_path in images[:self.k_shot]:\n    #             image = transforms.Resize(size=28)(Image.open(img_path))\n    #             image = np.array(image)\n    #             image = np.expand_dims(image / 255., axis=0)\n    #             support_data.append((image, label))\n\n    #         # Read query set\n    #         for img_path in images[self.k_shot:]:\n    #             image = transforms.Resize(size=28)(Image.open(img_path))\n    #             image = np.array(image)\n    #             image = np.expand_dims(image / 255., axis=0)\n    #             query_data.append((image, label))\n\n    #     # shuffle support set\n    #     random.shuffle(support_data)\n    #     for data in support_data:\n    #         support_image.append(data[0])\n    #         support_label.append(data[1])\n\n    #     # shuffle query set\n    #     random.shuffle(query_data)\n    #     for data in query_data:\n    #         query_image.append(data[0])\n    #         query_label.append(data[1])\n\n    #     query_label = np.array(query_label)\n\n    #     # Now apply pseudo-labeling for unlabeled data (query set)\n    #     query_tensor = torch.tensor(query_image).float()\n        \n    #     # Generate pseudo-labels for unlabeled data in the query set\n    #     pseudo_labels, high_confidence = generate_pseudo_labels(self.classifier, query_tensor)\n\n    #     # Update labels for high-confidence pseudo-labeled samples\n    #     query_label[high_confidence] = pseudo_labels[high_confidence]\n\n    #     return np.array(support_image), np.array(support_label), np.array(query_image), query_label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_dataset(\n        train_data_path,\n        test_data_path,\n        n_way,\n        k_shot,\n        q_query,\n        model\n):\n    \"\"\"\n    Get maml dataset.\n    Args:\n        args: ArgumentParser\n\n    Returns: dataset\n    \"\"\"\n    train_dataset = OmniglotDatasetTrain(train_data_path, \n                                    n_way, \n                                    k_shot, \n                                    q_query, \n                                    classifier=model)\n    \n    valid_dataset = OmniglotDatasetTrain(train_data_path, \n                                    n_way, \n                                    k_shot, \n                                    q_query, \n                                    classifier=model)\n\n    test_dataset = OmniglotDataset(test_data_path, \n                                   n_way, \n                                   k_shot, \n                                   q_query, \n                                    )\n    \n    train_dataset, valid_dataset = spilt_train_valid(train_dataset, \n                                                     valid_dataset, \n                                                     valid_size)\n\n    return train_dataset, valid_dataset, test_dataset\n\n\ndef spilt_train_valid(train_dataset, valid_dataset, valid_set_size):\n    \"\"\"\n    Spilt train dataset into train and valid dataset according to the given size.\n    Args:\n        train_dataset: original train dataset\n        valid_dataset: spilted valid dataset to put into\n        valid_set_size: given size in terms of proportion\n    \n    Returns: spilted train and valid datasets\n    \"\"\"\n    valid_set_size = int(valid_set_size * len(train_dataset))\n    train_set_size = len(train_dataset) - valid_set_size\n\n    file_list = train_dataset.file_list\n    random.shuffle(file_list)\n    \n    train_dataset.file_list = file_list[:train_set_size]\n    valid_dataset.file_list = file_list[train_set_size:]\n\n    return train_dataset, valid_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(ConvBlock, self).__init__()\n        self.conv2d = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n        self.bn = nn.BatchNorm2d(out_ch)\n        self.relu = nn.ReLU()\n        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n        x = self.conv2d(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.max_pool(x)\n        return x\n\n\ndef ConvBlockFunction(input, w, b, w_bn, b_bn):\n    x = F.conv2d(input, w, b, padding=1)\n    x = F.batch_norm(x, running_mean=None, running_var=None, weight=w_bn, bias=b_bn, training=True)\n    x = F.relu(x)\n    output = F.max_pool2d(x, kernel_size=2, stride=2)\n\n    return output\n\n\nclass Classifier(nn.Module):\n    def __init__(self, in_ch, n_way):\n        super(Classifier, self).__init__()\n        self.conv1 = ConvBlock(in_ch, 64)\n        self.conv2 = ConvBlock(64, 64)\n        self.conv3 = ConvBlock(64, 64)\n        self.conv4 = ConvBlock(64, 64)\n        self.logits = nn.Linear(64, n_way)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = x.view(x.shape[0], -1)\n        x = self.logits(x)\n        return x\n\n    def functional_forward(self, x, params):\n        for block in [1, 2, 3, 4]:\n            x = ConvBlockFunction(\n                x,\n                params[f\"conv{block}.conv2d.weight\"],\n                params[f\"conv{block}.conv2d.bias\"],\n                params.get(f\"conv{block}.bn.weight\"),\n                params.get(f\"conv{block}.bn.bias\"),\n            )\n        x = x.view(x.shape[0], -1)\n        x = F.linear(x, params[\"logits.weight\"], params[\"logits.bias\"])\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def maml_train(model, \n               support_images,\n               support_labels,\n               query_images,\n               query_labels, \n               inner_step, \n               inner_lr,\n               optimizer, \n               loss_fn,\n               is_train=True):\n    \"\"\"\n    Train the model using MAML method.\n    Args:\n        model: Any model\n        support_images: several task support images\n        support_labels: several  support labels\n        query_images: several query images\n        query_labels: several query labels\n        inner_step: support data training step\n        inner_lr: inner\n        optimizer: optimizer\n        is_train: whether train\n\n    Returns: meta loss, meta accuracy\n    \"\"\"\n    meta_loss = []\n    meta_acc = []\n\n    # Get support set and query set data for one train task\n    for support_image, support_label, query_image, query_label \\\n        in zip(support_images, support_labels, query_images, query_labels):\n\n        fast_weights = collections.OrderedDict(model.named_parameters())\n        for _ in range(inner_step):\n            # Update weight\n            # logit: batch_num * n_way * 1\n            support_logit = model.functional_forward(support_image, fast_weights)\n            support_loss = loss_fn(support_logit, support_label)\n            grads = torch.autograd.grad(support_loss, \n                                        fast_weights.values(), \n                                        create_graph=True)\n            fast_weights = collections.OrderedDict((name, param - inner_lr * grads)\n                                                   for ((name, param), grads) \n                                                   in zip(fast_weights.items(), grads))\n\n        # Use trained weight to get query loss\n        query_logit = model.functional_forward(query_image, fast_weights)\n        query_prediction = torch.max(query_logit, dim=1)[1]\n\n        query_loss = loss_fn(query_logit, query_label)\n        query_acc = torch.eq(query_label, query_prediction).sum() / len(query_label)\n\n        meta_loss.append(query_loss)\n        meta_acc.append(query_acc.data.cpu().numpy())\n\n    meta_loss = torch.stack(meta_loss).mean()\n    meta_acc = np.mean(meta_acc)\n\n    if is_train:\n        optimizer.zero_grad()\n        meta_loss.backward()\n        optimizer.step()\n\n    return meta_loss, meta_acc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_tasks, valid_tasks, test_tasks = get_dataset(train_data_path,\n#                                                    test_data_path,\n#                                                    n_way,\n#                                                    k_shot,\n#                                                    q_query,\n#                                                    model=ssl_model)\n\n# train_loader = DataLoader(train_tasks, batch_size=meta_batch_size, \n#                             shuffle=True, drop_last=True,  num_workers=num_workers)\n\n# valid_loader = DataLoader(valid_tasks, batch_size=meta_batch_size, \n#                             shuffle=True, drop_last=True, num_workers=num_workers)\n\n# test_loader = DataLoader(test_tasks, batch_size=meta_batch_size, \n#                             shuffle=False, drop_last=True, num_workers=num_workers)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"maml_model = Classifier(in_ch=1, n_way=n_way)\nmaml_model.to(device)\noptimizer = optim.Adam(maml_model.parameters(), outer_lr)\nloss_fn = nn.CrossEntropyLoss().to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint = torch.load('/kaggle/input/maml-5way-1shot-model/maml-para.pt') \nmaml_model.load_state_dict(checkpoint)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # ====================== evaluate model ====================\n# model.load_state_dict(torch.load('maml-para.pt'))\n# test_acc = []\n# test_loss = []\n\n# test_bar = tqdm(test_loader)\n# model.eval()\n# for support_images, support_labels, query_images, query_labels in test_bar:\n#     test_bar.set_description(\"Testing\")\n\n#     support_images = support_images.float().to(device)\n#     support_labels = support_labels.long().to(device)\n#     query_images = query_images.float().to(device)\n#     query_labels = query_labels.long().to(device)\n\n#     loss, acc = maml_train(model, \n#                     support_images, \n#                     support_labels, \n#                     query_images, \n#                     query_labels,\n#                     eval_inner_step, \n#                     inner_lr,\n#                     optimizer, \n#                     loss_fn, \n#                     is_train=False)\n#     test_loss.append(loss.item())\n#     test_acc.append(acc)\n\n# test_loss = np.mean(test_loss)\n# test_acc = np.mean(test_acc)\n# print('Meta Test Loss: {:.3f}, Meta Test Acc: {:.2f}%'.format(test_loss, 100 * test_acc))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\n\n# Dictionary to hold the gradients and activations\ngradients = []\nactivations = []\n\ndef save_gradients(grad):\n    global gradients\n    gradients = grad\n\ndef save_activations(module, input, output):\n    global activations\n    activations = output\n\ndef register_hooks(model):\n    # Register hooks for the last convolutional layer\n    last_conv_layer = model.conv4  # ConvBlock of the last layer (you can change this depending on your model structure)\n    \n    last_conv_layer.conv2d.register_forward_hook(save_activations)\n    last_conv_layer.conv2d.register_backward_hook(save_gradients)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def grad_cam(model, image, target_class=None):\n    model.eval()\n    image = image.unsqueeze(0).to(device)  # Add batch dimension and move to device (GPU/CPU)\n\n    # Forward pass through the model\n    output = model(image)\n\n    if target_class is None:\n        target_class = output.argmax(dim=1).item()  # Use the class with the highest score\n\n    # Zero gradients to prepare for backward pass\n    model.zero_grad()\n\n    # Backward pass: calculate gradients of the target class score w.r.t. the last convolutional layer\n    target_score = output[0, target_class]\n    target_score.backward()\n\n    # Get the gradients and activations\n    global gradients, activations\n\n    # Get the weights of the convolutional layer for the target class\n    weights = torch.mean(gradients, dim=[0, 2, 3])  # Average over all spatial locations\n\n    # Get the activations of the convolutional layer\n    cams = torch.zeros(activations.shape[2:], dtype=torch.float32).to(device)\n\n    # Compute the weighted sum of the activations\n    for i in range(weights.shape[0]):\n        cams += weights[i] * activations[0, i, :, :]\n\n    # Apply ReLU to the weighted sum (Grad-CAM is non-negative)\n    cams = F.relu(cams)\n\n    # Normalize the cam\n    cams = cams - cams.min()\n    cams = cams / cams.max()\n\n    # Convert to numpy and resize to match input image size\n    cams = cams.cpu().detach().numpy()\n    cams = cv2.resize(cams, (image.shape[2], image.shape[3]))  # Resize to image size\n    return cams\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_grad_cam(image, cams, target_class, label=None, save_path=None):\n    # Convert the input image from tensor to numpy (remove batch dimension)\n    image = image.squeeze().cpu().detach().numpy()\n\n    # Normalize the image to [0, 1]\n    image = (image - image.min()) / (image.max() - image.min())\n\n    # Convert the heatmap to a 2D array\n    heatmap = np.uint8(255 * cams)  # Scale between 0 and 255\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)  # Apply color map to heatmap\n\n    # Resize heatmap to match the original image size\n    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n\n    # Superimpose heatmap on the image\n    superimposed_img = heatmap * 0.4 + np.repeat(image[:, :, np.newaxis], 3, axis=2) * 255\n\n    # Convert to uint8 for visualization\n    superimposed_img = np.uint8(np.clip(superimposed_img, 0, 255))\n\n    # Display the image with heatmap overlay\n    plt.figure(figsize=(10, 10))\n    plt.imshow(superimposed_img)\n    if label:\n        plt.title(f\"True label: {label}, Predicted class: {target_class}\")\n    else:\n        plt.title(f\"Predicted class: {target_class}\")\n    plt.axis('off')\n    plt.show()\n    \n    if save_path:\n        plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n        print(f\"Saved Grad-CAM image to {save_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# maml_model = Classifier(in_ch=1, n_way=n_way)\n# maml_model.load_state_dict(torch.load('maml_model.pth'))\n# maml_model.to(device)\n\n# # Register the hooks on the model\n# register_hooks(maml_model)\n\n# # Sample a few test images (from the test set)\n# test_loader = DataLoader(test_tasks, batch_size=5, shuffle=False, drop_last=True)\n\n# for support_images, support_labels, query_images, query_labels in test_loader:\n#     # Choose a batch of images (for Grad-CAM, we need a single image, so select one image from the batch)\n#     query_images = query_images.to(device)\n\n#     for i in range(5):  # Show Grad-CAM for 5 images\n#         image = query_images[i]\n#         label = query_labels[i].item()\n\n#         # Compute the Grad-CAM heatmap for the image\n#         cams = grad_cam(maml_model, image, target_class=label)\n\n#         # Visualize the heatmap overlaid on the input image\n#         visualize_grad_cam(image, cams, target_class=label, label=label)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Make sure the model is in evaluation mode and the hooks are registered\nmaml_model.eval()  # Ensure the model is in evaluation mode (important for BatchNorm, Dropout)\n\n# Register the hooks (we do this after loading the model)\nregister_hooks(maml_model)\n\n# Load the test data (you already have the test_loader from the code)\ntest_loader = DataLoader(test_tasks, batch_size=5, shuffle=False, drop_last=True)\n\n# Now let's visualize Grad-CAM for a few test images\nfor support_images, support_labels, query_images, query_labels in test_loader:\n    # For demonstration, we'll take the first 5 query images from the test batch\n    query_images = query_images.to(device)  # Move images to the correct device (GPU/CPU)\n\n    for i in range(5):  # We can visualize Grad-CAM for 5 images\n        image = query_images[i]  # Select the i-th image\n        label = query_labels[i].item()  # Get the label for the image\n\n        # Compute Grad-CAM for the image\n        cams = grad_cam(maml_model, image, target_class=label)\n        save_path = os.path.join(output_dir, f\"grad_cam_image_{i}_label_{label}.png\")\n        # Visualize the Grad-CAM output\n        visualize_grad_cam(image, cams, target_class=label, label=label, save_path=save_path)\n\n    break  # We only need to process one batch for now (remove this line to process the entire test set)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}